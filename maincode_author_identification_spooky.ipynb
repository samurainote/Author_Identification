{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Identification with Natural Language Processing\n",
    "### - 自然言語処理を用いた「著者判定」-  2019年1月15日\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://images.unsplash.com/photo-1524995997946-a1c2e315a42f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【ゴール】  \n",
    "・著者判定\n",
    "\n",
    "【データセット】  \n",
    "・Spooky\n",
    "\n",
    "【カバー内容】  \n",
    "・TF-IDF  \n",
    "・ロジスティック回帰  \n",
    "・ナイーブベイズ  \n",
    "・サポートベクターマシン  \n",
    "・アンサンブル学習  \n",
    "・XG-Boost（勾配降下法）  \n",
    "・Grid Search  \n",
    "・Word Vectors  \n",
    "・LSTM  \n",
    "・GRU  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Spooky\"のデータセットをロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv') \n",
    "test = pd.read_csv('test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの概要を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of author in train dataset : (19579, 3)\n",
      "The size of author in test dataset : (8392, 2)\n",
      "The unique number of author in dataset : 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The size of author in train dataset : {}\".format(train.shape))\n",
    "print(\"The size of author in test dataset : {}\".format(test.shape))\n",
    "print(\"The unique number of author in dataset : {}\".format(train[\"author\"].nunique()))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多クラス分類における対数損失を評価指標としてモデルを評価する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnのラベルエンコーダーを使ってデータ型を変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データとテストデータに分割します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    train.text.values, y, \n",
    "    stratify=y, \n",
    "    random_state=42, \n",
    "    test_size=0.1, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. シンプルな分類器\n",
    "\n",
    "### 1-1. TF-IDF (Term Frequency - Inverse Document Frequency)  \n",
    "による重要単語抽出を用いた著者推定を行う。\n",
    "予測モデルは、ロジスティック回帰による多クラス分類（３クラス）を用います。  \n",
    "\n",
    "【手順】　　  \n",
    "・TF-IDFで各著者の文章の特徴を最も表す単語を抽出（ソート）する関数を作成    \n",
    "・学習データとテストデータの説明変数に上記の関数を噛ませる  \n",
    "・ロジスティック回帰に学習データを噛ませて予測モデルを作成   \n",
    "・モデルの評価指標として \"multi-class logarithmic loss\" を用いる  \n",
    "\n",
    "TfidfVectorizerを使うと、文章を特徴づける単語を探すことができる。  \n",
    "ロジックは、文章内に出現する単語の「出現頻度」と「希少性」を掛け合わせた値Tfidfを算出するアルゴリズム。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出現回数が３回以上の単語を対象に、説明変数を生成する\n",
    "tfidf = TfidfVectorizer(min_df=3, max_features=None, \n",
    "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "\n",
    "# 文章内の単語のTfidf値を取得\n",
    "tfidf.fit(list(xtrain) + list(xtest))\n",
    "xtrain_tfidf = tfidf.transform(xtrain)\n",
    "xtest_tfidf = tfidf.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロジスティック回帰では、活性化関数をロジット関数 logit(x) としているため、デフォルトでは二値分類問題にしか適用できない。  \n",
    "多クラス分類問題に拡張するとき、多クラスのラベルを表現するための式を導入し、活性化関数を変更する必要がある。  \n",
    "機械学習の分野において、多クラスを表現するには one-hot 表現が一般的に使われている。  \n",
    "この時、活性化関数はソフトマックス関数にする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.infiniteloop.co.jp/blog/wp-content/uploads/2017/12/multi-to-binary-classification-640x165.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数損失 multi-class logarithmic loss : 0.617 \n"
     ]
    }
   ],
   "source": [
    "# ロジスティック回帰をTfidf上で行う\n",
    "lr = LogisticRegression(C=1.0)\n",
    "lr.fit(xtrain_tfidf, ytrain)\n",
    "predictions = lr.predict_proba(xtest_tfidf)\n",
    "\n",
    "# multi-class logarithmic loss は0に近いほどいい\n",
    "print (\"対数損失 multi-class logarithmic loss : %0.3f \" % multiclass_logloss(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【結論】  \n",
    "・TF-IDFのみでは、あまり精度が出なかった  \n",
    "\n",
    "### 1-2. CountVectorizer による特徴量生成\n",
    "  \n",
    "TF-IDFでは各単語の出現回数に対して、文章全体における希少性も考慮した。  \n",
    "今度はCountVectorizerを用いて純粋に単語の出現回数を基準に各著者ごとの特徴を作り出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3))\n",
    "\n",
    "cv.fit(list(xtrain) + list(xtest))\n",
    "xtrain_cv = cv.transform(xtrain)\n",
    "xtest_cv = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数損失 multi-class logarithmic loss : 0.457 \n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1.0)\n",
    "lr.fit(xtrain_cv, ytrain)\n",
    "predictions = lr.predict_proba(xtest_cv)\n",
    "\n",
    "print(\"対数損失 multi-class logarithmic loss : %0.3f \" % multiclass_logloss(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Naive Bayes - 単純ベイズ分類器 -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://camo.qiitausercontent.com/77351cc8720301944b086011836dca40ef0f65ca/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3136313639312f30323863323264392d346339612d653065322d393565622d3063386361343931663936372e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数損失 multi-class logarithmic loss : 0.574 \n"
     ]
    }
   ],
   "source": [
    "# まずはTfidfによって生成した特徴を説明変数とする\n",
    "nbc = MultinomialNB()\n",
    "nbc.fit(xtrain_tfidf, ytrain)\n",
    "predictions = nbc.predict_proba(xtest_tfidf)\n",
    "print (\"対数損失 multi-class logarithmic loss : %0.3f \" % multiclass_logloss(ytest, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数損失 multi-class logarithmic loss : 0.755 \n"
     ]
    }
   ],
   "source": [
    "# 次はCountVectorizerによって生成した特徴を説明変数とする\n",
    "nbc = MultinomialNB()\n",
    "nbc.fit(xtrain_cv, ytrain)\n",
    "predictions = nbc.predict_proba(xtest_cv)\n",
    "print (\"対数損失 multi-class logarithmic loss : %0.3f \" % multiclass_logloss(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Singular Value Decomposition - 特異値分解 -\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特異値分解によってベクトル化した単語の次元を重要度の低い順から削除する\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfidf)\n",
    "xtrain_svd = svd.transform(xtrain_tfidf)\n",
    "xtest_svd = svd.transform(xtest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特徴量の正規化と標準化\n",
    "\n",
    "\n",
    "特徴量の尺度について。特徴量の尺度を揃えなさい、揃え方には正規化と標準化がある。  \n",
    "多くの機械学習アルゴリズムでは標準化、つまり標準偏差で割ることが実用的とのこと。  \n",
    "Scikit-learnでは、StandardScaler（標準化）、 MinMaxScaler（正規化）である。  \n",
    "  \n",
    "・fit パラメータ（平均や標準偏差 etc）計算  \n",
    "・transform パラメータをもとにデータ変換  \n",
    "・fit_transform パラメータ計算とデータ変換をまとめて実行  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://i.stack.imgur.com/rKSuk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次元削減後の次元を標準化する\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "scl_xtrain_svd = scl.transform(xtrain_svd)\n",
    "scl_xtest_svd = scl.transform(xtest_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではサポートベクターマシン分類を予測モデルとして適応する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数損失 multi-class logarithmic loss : 0.700 \n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=1.0, probability=True)\n",
    "svc.fit(scl_xtrain_svd, ytrain)\n",
    "predictions = svc.predict_proba(scl_xtest_svd)\n",
    "\n",
    "print (\"対数損失 multi-class logarithmic loss : %0.3f \" % multiclass_logloss(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search ハイパーパラメータの最適化  \n",
    "   \n",
    "正則化やガンマ分布など過学習や未学習の調整を自動で行う  \n",
    "\n",
    "refference : http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【手順】    \n",
    "・スコアラーの作成　　　  \n",
    "・SVDによる次元削減      \n",
    "・次元の標準化      \n",
    "・ロジスティック回帰    \n",
    "    \n",
    "ここでは、scikit-learnの\"make_scorer\"関数を使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better = False, needs_proba = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量選択、前処理（スケール調整）、パラメータ設定、機械学習アルゴリズムの選択…という一連のプロセスを一つのEstimatorとして定義できると、API化が容易になったり、あるいはGridSearchによる自動化ができる。    \n",
    "    \n",
    "Pipelineを用いると、以下の点が付け加えられる。      \n",
    "    \n",
    "・特徴量選択や前処理のパラメータ設定の自動化   \n",
    "・機械学習アルゴリズムや前処理メソッドの自動選択  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD()\n",
    "scl = preprocessing.StandardScaler()\n",
    "lr = LogisticRegression()\n",
    "clf = pipeline.Pipeline([('svd', svd), ('scl', scl), ('lr', lr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「正則化」(regularization / penalized regression)のとは、モデル最適化のための誤差関数ED(w)に対して、ペナルティ項E(w)を加える。\n",
    "ED(w)+λE(w)を最小化するように最適化問題を解く問題に置き換えることで、「ほどよく」過学習を避けつつ穏当なモデルに落ち付かせることを目的としている。  \n",
    "   \n",
    "・L1正則化回帰はLasso回帰   \n",
    "・L2正則化回帰はRidge回帰   \n",
    "・L1 / L2正則化項はElastic net正則化   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  2.3min remaining:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.6min finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベストスコア: -0.691\n",
      "Best parameters set:\n",
      "\tlr__C: 1.0\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# フィッティング\n",
    "model.fit(xtrain_tfidf, ytrain)\n",
    "print(\"ベストスコア: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単純ベイズを分類器として使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.4s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.451\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfidf, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors - 単語のベクトル表現 -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ディープラーニング TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アンサンブル学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
